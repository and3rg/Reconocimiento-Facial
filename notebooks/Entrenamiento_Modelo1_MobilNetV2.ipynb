{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera prueba - Test de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "basePath = '../data'  # Ajusta la ruta si es necesario\n",
    "train_dir = os.path.join(basePath, 'preprocessed', 'train')  # Ruta para entrenar\n",
    "val_dir = os.path.join(basePath, 'preprocessed', 'validation')  # Ruta para validar\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "num_classes = 15\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5120 images belonging to 15 classes.\n",
      "Found 1200 images belonging to 15 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.7678 - loss: 7.2957 - val_accuracy: 0.8623 - val_loss: 1.0648\n",
      "Epoch 2/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.8125 - val_loss: 1.0741\n",
      "Epoch 3/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 332ms/step - accuracy: 0.9535 - loss: 0.5714 - val_accuracy: 0.8505 - val_loss: 0.5397\n",
      "Epoch 4/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.8750 - val_loss: 0.5092\n",
      "Epoch 5/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 346ms/step - accuracy: 0.9685 - loss: 0.2736 - val_accuracy: 0.9713 - val_loss: 0.2327\n",
      "Epoch 6/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1609\n",
      "Epoch 7/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 352ms/step - accuracy: 0.9792 - loss: 0.2065 - val_accuracy: 0.9840 - val_loss: 0.1707\n",
      "Epoch 8/30\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1168\n",
      "Epoch 9/30\n",
      "\u001b[1m 87/160\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 312ms/step - accuracy: 0.9666 - loss: 0.2402"
     ]
    }
   ],
   "source": [
    "# Generadores de datos con aumento de datos\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Cargar la red base preentrenada (MobileNetV2)\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=img_size + (3,),\n",
    "    include_top=False,  # No incluir la capa densa de clasificación\n",
    "    weights='imagenet'  # Cargar los pesos preentrenados en ImageNet\n",
    ")\n",
    "\n",
    "# Descongelar algunas capas del modelo base\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:  # Congelar todas las capas excepto las últimas 4\n",
    "    layer.trainable = False\n",
    "\n",
    "# Añadir capas superiores para nuestra clasificación\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Crear el modelo final\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitorea la pérdida de validación\n",
    "    patience=10,         # Número de épocas sin mejora antes de detener el entrenamiento\n",
    "    restore_best_weights=True  # Restaura los mejores pesos al final del entrenamiento\n",
    ")\n",
    "\n",
    "# Callback para registrar el tiempo por época\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Entrenar el modelo con Early Stopping y el callback de tiempo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, time_callback]\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Pérdida en validación: {loss}\")\n",
    "print(f\"Precisión en validación: {accuracy}\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('../models/reconocimiento_facial_15_personasV2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluacion del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar el rendimiento del modelo\n",
    "def plot_training(history, epoch_times):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    # Ensure all lists have the same length\n",
    "    min_length = min(len(acc), len(val_acc), len(loss), len(val_loss), len(epoch_times))\n",
    "    acc = acc[:min_length]\n",
    "    val_acc = val_acc[:min_length]\n",
    "    loss = loss[:min_length]\n",
    "    val_loss = val_loss[:min_length]\n",
    "    epoch_times = epoch_times[:min_length]\n",
    "    \n",
    "    epochs_range = range(min_length)  # Ensure epochs_range matches the length of history data\n",
    "\n",
    "    # Graficar la precisión\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(epochs_range, acc, 'o-',label='Precisión de Entrenamiento')\n",
    "    plt.plot(epochs_range, val_acc, 'o-',label='Precisión de Validación')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Precisión durante el Entrenamiento y Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "\n",
    "    # Graficar la pérdida\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(epochs_range, loss, label='Pérdida de Entrenamiento')\n",
    "    plt.plot(epochs_range, val_loss, label='Pérdida de Validación')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Pérdida durante el Entrenamiento y Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "\n",
    "    # Graficar el tiempo por época\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(epochs_range, epoch_times, 'o-', label='Tiempo por Época', color='green')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Tiempo por Época durante el Entrenamiento')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "\n",
    "    # Mostrar las gráficas\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Llamar la función para graficar el rendimiento\n",
    "plot_training(history, time_callback.times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
